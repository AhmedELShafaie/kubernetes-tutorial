= Taints and Affinity
include::_attributes.adoc[]

So far, when we deployed any Pod in the Kubernetes cluster, it was run on any node that met the requirements (ie memory requirements, CPU requirements, ...)

However, in Kubernetes there are two concepts that allow you to further configure the scheduler, so that Pods are assigned to Nodes following some business criteria.

== Preparation

include::https://raw.githubusercontent.com/redhat-developer-demos/rhd-tutorial-common/master/minikube-multinode.adoc[]

== Taints and Tolerations

A Taint is applied to a Kubernetes Node that signals the scheduler to avoid or not schedule certain Pods.

A Toleration is applied to a Pod definition and provides an exception to the taint.

Let's describe the current nodes, in this case as an OpenShift cluster is used, you can see several nodes:

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl describe nodes | egrep "Name:|Taints:"
----

[.console-output]
[source,bash]
----
Name:               ip-10-0-136-107.eu-central-1.compute.internal
Taints:             node-role.kubernetes.io/master:NoSchedule
Name:               ip-10-0-140-186.eu-central-1.compute.internal
Taints:             <none>
Name:               ip-10-0-141-128.eu-central-1.compute.internal
Taints:             <none>
Name:               ip-10-0-146-109.eu-central-1.compute.internal
Taints:             <none>
Name:               ip-10-0-150-226.eu-central-1.compute.internal
Taints:             <none>
----

Notice that in this case, the `master` node contains a taint which blocks your application Pods from being scheduled there.

=== Taints

Let's add a taint to all nodes:

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl taint nodes --all=true color=blue:NoSchedule
----

[.console-output]
[source,bash]
----
node/ip-10-0-136-107.eu-central-1.compute.internal tainted
node/ip-10-0-140-186.eu-central-1.compute.internal tainted
node/ip-10-0-141-128.eu-central-1.compute.internal tainted
node/ip-10-0-146-109.eu-central-1.compute.internal tainted
node/ip-10-0-150-226.eu-central-1.compute.internal tainted
node/ip-10-0-155-122.eu-central-1.compute.internal tainted
node/ip-10-0-162-206.eu-central-1.compute.internal tainted
node/ip-10-0-168-102.eu-central-1.compute.internal tainted
node/ip-10-0-175-64.eu-central-1.compute.internal tainted
----

The color=blue is simply a key=value pair to identify the taint and NoSchedule is the specific effect.

Now deploy a new pod.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl apply -f apps/kubefiles/myboot-deployment.yml

kubectl get pods
----

The pod will remain in Pending status as it has no schedulable Node available.

[.console-output]
[source,bash]
----
NAME                     READY   STATUS    RESTARTS   AGE
myboot-7f889dd6d-n5z55   0/1     Pending   0          55s
----

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl describe pod myboot-7f889dd6d-n5z55
----

[.console-output]
[source,bash]
----
Name:           myboot-7f889dd6d-n5z55
Namespace:      kubetut
Priority:       0
Node:           <none>
Labels:         app=myboot
                pod-template-hash=7f889dd6d
Annotations:    openshift.io/scc: restricted
Status:         Pending

Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
Events:
  Type     Reason            Age        From               Message
  ----     ------            ----       ----               -------
  Warning  FailedScheduling  <unknown>  default-scheduler  0/9 nodes are available: 9 node(s) had taints that the pod didn't tolerate.
  Warning  FailedScheduling  <unknown>  default-scheduler  0/9 nodes are available: 9 node(s) had taints that the pod didn't tolerate.
----

Now let's remove one taint from one node and see what's happening:

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl get nodes
----

[.console-output]
[source,bash]
----
NAME                                            STATUS   ROLES    AGE   VERSION
ip-10-0-136-107.eu-central-1.compute.internal   Ready    master   20h   v1.16.2
ip-10-0-140-186.eu-central-1.compute.internal   Ready    worker   20h   v1.16.2
ip-10-0-141-128.eu-central-1.compute.internal   Ready    worker   18h   v1.16.2
ip-10-0-146-109.eu-central-1.compute.internal   Ready    worker   18h   v1.16.2
ip-10-0-150-226.eu-central-1.compute.internal   Ready    worker   20h   v1.16.2
ip-10-0-155-122.eu-central-1.compute.internal   Ready    master   20h   v1.16.2
ip-10-0-162-206.eu-central-1.compute.internal   Ready    worker   20h   v1.16.2
ip-10-0-168-102.eu-central-1.compute.internal   Ready    master   20h   v1.16.2
ip-10-0-175-64.eu-central-1.compute.internal    Ready    worker   18h   v1.16.2
----

Pick one node.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl taint node ip-10-0-140-186.eu-central-1.compute.internal color:NoSchedule-
----

[.console-output]
[source,bash]
----
node/ip-10-0-140-186.eu-central-1.compute.internal  untainted
----

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl describe nodes | egrep "Name:|Taints:"
----

[.console-output]
[source,bash]
----
Name:               ip-10-0-136-107.eu-central-1.compute.internal
Taints:             node-role.kubernetes.io/master:NoSchedule
Name:               ip-10-0-140-186.eu-central-1.compute.internal
Taints:             <none>
Name:               ip-10-0-141-128.eu-central-1.compute.internal
Taints:             color=blue:NoSchedule
Name:               ip-10-0-146-109.eu-central-1.compute.internal
Taints:             color=blue:NoSchedule
----

And you should see the Pending pod scheduled to the newly untained node.  

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl get pods
----

[.console-output]
[source,bash]
----
NAME                     READY   STATUS    RESTARTS   AGE
myboot-7f889dd6d-n5z55   1/1     Running   0          11m
----

[TIP]
====
You can use `kubectl get pods -o wide` to see the node upon which the pod is scheduled.

====


==== Clean Up

Undeploy the myboot deployment and add again the taint to the node:

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl delete -f apps/kubefiles/myboot-deployment.yml

kubectl taint node ip-10-0-140-186.eu-central-1.compute.internal color=blue:NoSchedule
----

=== Tolerations

Let's create a Pod but containing a toleration, so it can be scheduled to a tainted node.

[source, yaml]
----
spec:
  tolerations:
  - key: "color"
    operator: "Equal"
    value: "blue"
    effect: "NoSchedule"
  containers:
  - name: myboot
    image: quay.io/rhdevelopers/myboot:v1
----

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl apply -f apps/kubefiles/myboot-toleration.yaml

kubectl get pods
----

[.console-output]
[source,bash]
----
NAME                      READY   STATUS    RESTARTS   AGE
myboot-84b457458b-mbf9r   1/1     Running   0          3m18s
----

Now, although all nodes contain a taint, the Pod is scheduled and run as we defined a tolerance against color=blue taint.

==== Clean Up

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl delete -f apps/kubefiles/myboot-toleration.yaml
----

=== NoExecution Taint

So far, you've seen the `NoSchedule` taint effect which means that newly created Pods will not be scheduled there unless they have an overriding toleration.
But notice that if we add this taint to a node that already has running/scheduled Pods, this taint will not terminate them.

Let's change that by using `NoExecution` effect. 

First of all, let's remove all previous taints.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl taint nodes --all=true color=blue:NoSchedule-
----

Then deploy a service:

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl apply -f apps/kubefiles/myboot-deployment.yml

kubectl get pods
----

[.console-output]
[source,bash]
----
NAME                     READY   STATUS    RESTARTS   AGE
myboot-7f889dd6d-bkfg5   1/1     Running   0          16s
----

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl get pod myboot-7f889dd6d-bkfg5 -o json | jq '.spec.nodeName'
----

[.console-output]
[source,bash]
----
"ip-10-0-146-109.eu-central-1.compute.internal"
----

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl taint node ip-10-0-146-109.eu-central-1.compute.internal color=blue:NoExecute

kubectl get pods
----

[.console-output]
[source,bash]
----
NAME                     READY   STATUS        RESTARTS   AGE
myboot-7f889dd6d-8fm2v   1/1     Running       0          14s
myboot-7f889dd6d-bkfg5   1/1     Terminating   0          5m51s
----

If you have more nodes available then the Pod is terminated and deployed onto another node, if it is not the case, then the Pod will remain in _Pending_ status.

You can watch this "rescheduling" occur with use of  `-o wide`

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
watch kubectl get pods -o wide
----

[.console-output]
[source,bash]
----
NAME                     READY   STATUS        RESTARTS   AGE     IP           NODE     
myboot-7f889dd6d-b9tks   1/1     Running       0          6s      10.88.0.5    devnation-m02   
myboot-7f889dd6d-l897f   1/1     Terminating   0          9m11s   172.17.0.4   devnation       
----


==== Clean Up

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl delete -f apps/kubefiles/myboot-deployment.yml
----

And remove the NoExecute taint 

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl taint node ip-10-0-146-109.eu-central-1.compute.internal color=blue:NoExecute-
----


== Affinity & Anti-Affinity

There is another way of changing where Pods are scheduled using Node/Pod Affinity and Anti-affinity.
You can create rules that not only ban where Pods can run but also to favor where they should be run.

In addition to creating affinities between Pods and Nodes, you can also create affinities between Pods.  You can decide that a group of Pods should be always be deployed together on the same node(s).
Reasons such as significant network communication between Pods and you want to avoid external network calls or perhaps shared storage devices.

=== Node Affinity

Let's deploy a new pod with a node affinity:

[source, yaml]
----
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: color
            operator: In
            values:
            - blue
      containers:
      - name: myboot
        image: quay.io/rhdevelopers/myboot:v1
----

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl apply -f apps/kubefiles/myboot-node-affinity.yml

kubectl get pods
----

[.console-output]
[source,bash]
----
NAME                           READY   STATUS    RESTARTS   AGE
myboot-54d788fdc8-f6wks        0/1     Pending   0          13s
----

Let's create a label on a node matching the affinity expression:

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl get nodes
----

[.console-output]
[source,bash]
----
NAME                                            STATUS   ROLES    AGE   VERSION
ip-10-0-136-107.eu-central-1.compute.internal   Ready    master   26h   v1.16.2
ip-10-0-140-186.eu-central-1.compute.internal   Ready    worker   26h   v1.16.2
ip-10-0-141-128.eu-central-1.compute.internal   Ready    worker   25h   v1.16.2
ip-10-0-146-109.eu-central-1.compute.internal   Ready    worker   25h   v1.16.2
ip-10-0-150-226.eu-central-1.compute.internal   Ready    worker   26h   v1.16.2
ip-10-0-155-122.eu-central-1.compute.internal   Ready    master   26h   v1.16.2
ip-10-0-162-206.eu-central-1.compute.internal   Ready    worker   26h   v1.16.2
ip-10-0-168-102.eu-central-1.compute.internal   Ready    master   26h   v1.16.2
ip-10-0-175-64.eu-central-1.compute.internal    Ready    worker   25h   v1.16.2
----

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl label nodes ip-10-0-175-64.eu-central-1.compute.internal color=blue
----

[.console-output]
[source,bash]
----
node/ip-10-0-175-64.eu-central-1.compute.internal labeled
----

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl get pods
----

[.console-output]
[source,bash]
----
NAME                          READY   STATUS    RESTARTS   AGE
myboot-54d788fdc8-f6wks       1/1     Running   0          7m57s
----

Let's delete the label from the node:

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl label nodes ip-10-0-175-64.eu-central-1.compute.internal color-

kubectl get pods
----

[.console-output]
[source,bash]
----
NAME                         READY   STATUS    RESTARTS   AGE
myboot-54d788fdc8-f6wks      1/1     Running   0          7m57s
----

As with taints, the rule is set during the scheduling phase, therefore, the Pod is not removed.

This is an example of a _hard_ rule, if the Kubernetes scheduler does not find any node with the required label then the Pod reminds in _Pending_ state.
There is also a way to create a _soft_ rule, where the Kubernetes scheduler attempts to match the rules but if it can not then the Pod is scheduled to any node.  In the example below, you can see the use of the word _preferred_ vs _required_.

[source, yaml]
----
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: color
            operator: In
            values:
            - blue
----

==== Clean Up

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl delete -f apps/kubefiles/myboot-node-affinity.yml
----

=== Pod Affinity/Anti-Affinity

Let's deploy a new pod with a Pod Affinity:

[source, yaml]
----
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - topologyKey: kubernetes.io/hostname # <1>
        labelSelector: 
          matchExpressions:
          - key: app
            operator: In
            values:
            - myboot # <2>
  containers:
----
<1> The node label key. If two nodes are labeled with this key and have identical values, the scheduler treats both nodes as being in the same topology. In this case, `hostname` is a label that is different for each node.
<2> The affinity is with Pods labeled with `app=myboot`.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl apply -f apps/kubefiles/myboot-pod-affinity.yml

kubectl get pods
----

[.console-output]
[source,bash]
----
NAME                                                              READY   STATUS    RESTARTS   AGE
myboot2-784bc58c8d-j2l74                                          0/1     Pending   0          19s
----

The `myboot2` Pod is pending as couldn't find any Pod matching the affinity rule.
Let's deploy `myboot` application labeled with `app=myboot`.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl apply -f apps/kubefiles/myboot-deployment.yml

kubectl get pods
----

[.console-output]
[source,bash]
----
NAME                                                              READY   STATUS    RESTARTS   AGE
myboot-7f889dd6d-tr7gr                                            1/1     Running   0          3m27s
myboot2-64566b697b-snm7p                                          1/1     Running   0          18s
----

Now both applications are running in the same node:

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl get pod myboot-7f889dd6d-tr7gr -o json | jq '.spec.nodeName'
----

[.console-output]
[source,bash]
----
"ip-10-0-146-109.eu-central-1.compute.internal"
----

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl get pod myboot2-64566b697b-snm7p -o json | jq '.spec.nodeName'
----

[.console-output]
[source,bash]
----
"ip-10-0-146-109.eu-central-1.compute.internal"
----

What you've seen here is a _hard_ rule, you can use a "soft" rules as well in Pod Affinity.

[source, yaml]
----
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        podAffinityTerm:
          topologyKey: kubernetes.io/hostname 
          labelSelector:
            matchExpressions:  
            - key: app
              operator: In
              values:
              - myboot   
----

Anti-affinity is used to insure that two Pods do NOT run together on the same node.

[source, yaml]
----
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - topologyKey: kubernetes.io/hostname
        labelSelector: 
          matchExpressions:
          - key: app
            operator: In
            values:
            - myboot
----

Deploy a myboot3 with an anti-affinity rule

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl apply -f apps/kubefiles/myboot-pod-antiaffinity.yaml
----

And then use the `kubectl get pods -o wide` command to see which pods land on which nodes.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl get pods -o wide
----

[.console-output]
[source,bash]
----
NAME                       READY   STATUS    RESTARTS   AGE    IP          NODE
myboot-7f889dd6d-tr7gz     1/1     Running   0          4m27s  10.88.0.9   devnation-m02
myboot2-64566b697b-snm7p   1/1     Running   0          48s    10.88.0.10  devnation-m02 
myboot3-78656b637r-suy1t   1/1     Running   0          1s     172.17.0.2  devnation
----

`myboot3` Pod is deployed in a different node than the `myboot` Pod

==== Clean Up

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl delete -f apps/kubefiles/myboot-pod-affinity.yml
kubectl delete -f apps/kubefiles/myboot-pod-antiaffinity.yml
kubectl delete -f apps/kubefiles/myboot-deployment.yml
----